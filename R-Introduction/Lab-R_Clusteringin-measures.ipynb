{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing cluster validation statistics in R\n",
    "Required R packages\n",
    "\n",
    "The following R packages are required in this chapter:\n",
    "\n",
    "*    factoextra for data visualization\n",
    "*    fpc for computing clustering validation statistics\n",
    "*    NbClust for determining the optimal number of clusters in the data set.\n",
    "\n",
    "Install and load the packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing packages into ‘/home/iserina/R/x86_64-pc-linux-gnu-library/3.5’\n",
      "(as ‘lib’ is unspecified)\n",
      "also installing the dependencies ‘dendextend’, ‘prabclus’\n",
      "\n",
      "Warning message in install.packages(new.packages):\n",
      "“installation of package ‘prabclus’ had non-zero exit status”Warning message in install.packages(new.packages):\n",
      "“installation of package ‘fpc’ had non-zero exit status”Warning message in install.packages(new.packages):\n",
      "“installation of package ‘dendextend’ had non-zero exit status”Warning message in install.packages(new.packages):\n",
      "“installation of package ‘factoextra’ had non-zero exit status”"
     ]
    }
   ],
   "source": [
    "list.of.packages <- c(\"factoextra\", \"fpc\", \"NbClust\",\"clValid\")\n",
    "new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
    "if(length(new.packages)) install.packages(new.packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(factoextra): there is no package called ‘factoextra’\n",
     "output_type": "error",
     "traceback": [
      "Error in library(factoextra): there is no package called ‘factoextra’\nTraceback:\n",
      "1. library(factoextra)",
      "2. stop(txt, domain = NA)"
     ]
    }
   ],
   "source": [
    "library(factoextra)\n",
    "library(fpc)\n",
    "library(NbClust)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation\n",
    "\n",
    "We’ll use the built-in R data set iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluding the column \"Species\" at position 5\n",
    "df <- iris[, -5]\n",
    "# Standardize\n",
    "df <- scale(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering analysis\n",
    "\n",
    "We’ll use the function eclust() [enhanced clustering, in factoextra] which provides several advantages:\n",
    "\n",
    "*    It simplifies the workflow of clustering analysis\n",
    "*    It can be used to compute hierarchical clustering and partitioning clustering in a single line function call\n",
    "*    Compared to the standard partitioning functions (kmeans, pam, clara and fanny) which requires the user to specify the optimal number of clusters, the function eclust() computes automatically the gap statistic for estimating the right number of clusters.\n",
    "*    It provides silhouette information for all partitioning methods and hierarchical clustering\n",
    "*    It draws beautiful graphs using ggplot2\n",
    "\n",
    "The simplified format the eclust() function is as follow:\n",
    "\n",
    "eclust(x, FUNcluster = \"kmeans\", hc_metric = \"euclidean\", ...)\n",
    "\n",
    "\n",
    "\n",
    "*    x: numeric vector, data matrix or data frame\n",
    "*    FUNcluster: a clustering function including “kmeans”, “pam”, “clara”, “fanny”, “hclust”, “agnes” and “diana”. Abbreviation is allowed.\n",
    "*    hc_metric: character string specifying the metric to be used for calculating dissimilarities between observations. Allowed values are those accepted by the function dist() [including “euclidean”, “manhattan”, “maximum”, “canberra”, “binary”, “minkowski”] and correlation based distance measures [“pearson”, “spearman” or “kendall”]. Used only when FUNcluster is a hierarchical clustering function such as one of “hclust”, “agnes” or “diana”.\n",
    "*    …: other arguments to be passed to FUNcluster.\n",
    "\n",
    "The function eclust() returns an object of class eclust containing the result of the standard function used (e.g., kmeans, pam, hclust, agnes, diana, etc.).\n",
    "\n",
    "It includes also:\n",
    "\n",
    "*    cluster: the cluster assignment of observations after cutting the tree\n",
    "*    nbclust: the number of clusters\n",
    "*    silinfo: the silhouette information of observations\n",
    "*    size: the size of clusters\n",
    "*    data: a matrix containing the original or the standardized data (if stand = TRUE)\n",
    "*    gap_stat: containing gap statistics\n",
    "\n",
    "To compute a partitioning clustering, such as k-means clustering with k = 3, type this:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means clustering\n",
    "km.res <- eclust(df, \"kmeans\", k = 3, nstart = 25, graph = FALSE)\n",
    "# Visualize k-means clusters\n",
    "fviz_cluster(km.res, geom = \"point\", ellipse.type = \"norm\",\n",
    "             palette = \"jco\", ggtheme = theme_minimal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute a hierarchical clustering, use this:\n",
    "\n",
    "# Hierarchical clustering\n",
    "hc.res <- eclust(df, \"hclust\", k = 3, hc_metric = \"euclidean\", \n",
    "                 hc_method = \"ward.D2\", graph = FALSE)\n",
    "\n",
    "# Visualize dendrograms\n",
    "fviz_dend(hc.res, show_labels = FALSE,\n",
    "         palette = \"jco\", as.ggplot = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering validation\n",
    "\n",
    "# Silhouette plot\n",
    "\n",
    "Recall that the silhouette coefficient (Si) measures how similar an object i is to the the other objects in its own cluster versus those in the neighbor cluster. Si\n",
    "\n",
    "values range from 1 to - 1:\n",
    "\n",
    "*    A value of Si close to 1 indicates that the object is well clustered. In the other words, the object i is similar to the other objects in its group.\n",
    "* A value of Si close to -1 indicates that the object is poorly clustered, and that assignment to some other cluster would probably improve the overall results.\n",
    "\n",
    "It’s possible to draw silhouette coefficients of observations using the function fviz_silhouette() [factoextra package], which will also print a summary of the silhouette analysis output. To avoid this, you can use the option print.summary = FALSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fviz_silhouette(km.res, palette=\"jco\",ggtheme=theme_classic())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Silhouette information can be extracted as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette information\n",
    "silinfo <- km.res$silinfo\n",
    "names(silinfo)\n",
    "# Silhouette widths of each observation\n",
    "head(silinfo$widths[, 1:3], 10)\n",
    "# Average silhouette width of each cluster\n",
    "silinfo$clus.avg.widths\n",
    "# The total average (mean of all individual silhouette widths)\n",
    "silinfo$avg.width\n",
    "# The size of each clusters\n",
    "km.res$size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that several samples, in cluster 2, have a negative silhouette coefficient. This means that they are not in the right cluster. We can find the name of these samples and determine the clusters they are closer (neighbor cluster), as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette width of observation\n",
    "sil <- km.res$silinfo$widths[, 1:3]\n",
    "# Objects with negative silhouette\n",
    "neg_sil_index <- which(sil[, 'sil_width'] < 0)\n",
    "sil[neg_sil_index, , drop = FALSE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Validation statistics\n",
    "\n",
    "The function cluster.stats() [fpc package] and the function NbClust() [in NbClust package] can be used to compute Dunn index and many other cluster validation statistics or indices.\n",
    "\n",
    "The simplified format is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster.stats(d = NULL, clustering, al.clustering = NULL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*    d: a distance object between cases as generated by the dist() function\n",
    "*    clustering: vector containing the cluster number of each observation\n",
    "*    alt.clustering: vector such as for clustering, indicating an alternative clustering\n",
    "\n",
    "The function cluster.stats() returns a list containing many components useful for analyzing the intrinsic characteristics of a clustering:\n",
    "\n",
    "*    cluster.number: number of clusters\n",
    "*    cluster.size: vector containing the number of points in each cluster\n",
    "*    average.distance, median.distance: vector containing the cluster-wise within average/median distances\n",
    "*    average.between: average distance between clusters. We want it to be as large as possible\n",
    "*    average.within: average distance within clusters. We want it to be as small as possible\n",
    "*    clus.avg.silwidths: vector of cluster average silhouette widths. Recall that, the silhouette width is also an estimate of the average distance between clusters. Its value is comprised between 1 and -1 with a value of 1 indicating a very good cluster.\n",
    "*    within.cluster.ss: a generalization of the within clusters sum of squares (k-means objective function), which is obtained if d is a Euclidean distance matrix.\n",
    "*    dunn, dunn2: Dunn index\n",
    "*    corrected.rand, vi: Two indexes to assess the similarity of two clustering: the corrected Rand index and Meila’s VI\n",
    "\n",
    "All the above elements can be used to evaluate the internal quality of clustering.\n",
    "\n",
    "In the following sections, we’ll compute the clustering quality statistics for k-means. Look at the within.cluster.ss (within clusters sum of squares), the average.within (average distance within clusters) and clus.avg.silwidths (vector of cluster average silhouette widths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(fpc)\n",
    "# Statistics for k-means clustering\n",
    "km_stats <- cluster.stats(dist(df),  km.res$cluster)\n",
    "# Dun index\n",
    "km_stats$dunn\n",
    "\n",
    "# To display all statistics, type this:\n",
    "\n",
    "km_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External clustering validation\n",
    "\n",
    "Among the values returned by the function cluster.stats(), there are two indexes to assess the similarity of two clustering, namely the corrected Rand index and Meila’s VI.\n",
    "\n",
    "We know that the iris data contains exactly 3 groups of species.\n",
    "\n",
    "Does the K-means clustering matches with the true structure of the data?\n",
    "\n",
    "We can use the function cluster.stats() to answer to this question.\n",
    "\n",
    "Let start by computing a cross-tabulation between k-means clusters and the reference Species labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(iris$Species, km.res$cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that:\n",
    "\n",
    "*    All setosa species (n = 50) has been classified in cluster 1\n",
    "*    A large number of versicor species (n = 39 ) has been classified in cluster 3. Some of them ( n = 11) have been classified in cluster 2.\n",
    "*    A large number of virginica species (n = 36 ) has been classified in cluster 2. Some of them (n = 14) have been classified in cluster 3.\n",
    "\n",
    "It’s possible to quantify the agreement between Species and k-means clusters using either the corrected Rand index and Meila’s VI provided as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"fpc\")\n",
    "# Compute cluster stats\n",
    "species <- as.numeric(iris$Species)\n",
    "clust_stats <- cluster.stats(d = dist(df), \n",
    "                             species, km.res$cluster)\n",
    "# Corrected Rand index\n",
    "clust_stats$corrected.rand\n",
    "\n",
    "# VI\n",
    "clust_stats$vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The corrected Rand index provides a measure for assessing the similarity between two partitions, adjusted for chance. Its range is -1 (no agreement) to 1 (perfect agreement). Agreement between the specie types and the cluster solution is 0.62 using Rand index and 0.748 using Meila’s VI.\n",
    "\n",
    "The same analysis can be computed for both PAM and hierarchical clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agreement between species and pam clusters\n",
    "pam.res <- eclust(df, \"pam\", k = 3, graph = FALSE)\n",
    "table(iris$Species, pam.res$cluster)\n",
    "cluster.stats(d = dist(iris.scaled), \n",
    "              species, pam.res$cluster)$vi\n",
    "# Agreement between species and HC clusters\n",
    "res.hc <- eclust(df, \"hclust\", k = 3, graph = FALSE)\n",
    "table(iris$Species, res.hc$cluster)\n",
    "cluster.stats(d = dist(iris.scaled), \n",
    "              species, res.hc$cluster)$vi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "External clustering  validation, can be used to select suitable clustering algorithm for a given data set.\n",
    " \n",
    " \n",
    "# R function clValid()\n",
    "# Format\n",
    "\n",
    "The main function in clValid package is clValid():\n",
    "\n",
    "clValid(obj, nClust, clMethods = \"hierarchical\",\n",
    "        validation = \"stability\", maxitems = 600,\n",
    "        metric = \"euclidean\", method = \"average\")\n",
    "\n",
    "\n",
    "*    obj: A numeric matrix or data frame. Rows are the items to be clustered and columns are samples.\n",
    "*    nClust: A numeric vector specifying the numbers of clusters to be evaluated. e.g., 2:10\n",
    "*    clMethods: The clustering method to be used. Available options are “hierarchical”, “kmeans”, “diana”, “fanny”, “som”, “model”, “sota”, “pam”, “clara”, and “agnes”, with multiple choices allowed.\n",
    "*    validation: The type of validation measures to be used. Allowed values are “internal”, “stability”, and “biological”, with multiple choices allowed.\n",
    "*    maxitems: The maximum number of items (rows in matrix) which can be clustered.\n",
    "*    metric: The metric used to determine the distance matrix. Possible choices are “euclidean”, “correlation”, and “manhattan”.\n",
    "*    method: For hierarchical clustering (hclust and agnes), the agglomeration method to be used. Available choices are “ward”, “single”, “complete” and “average”.\n",
    "\n",
    "\n",
    "## Examples of usage\n",
    "### Data\n",
    "\n",
    "We’ll use mouse data [in clValid package ] which is an Affymetrix gene expression data of of mesenchymal cells from two distinct lineages (M and N). It contains 147 genes and 6 samples (3 samples for each lineage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(clValid)\n",
    "# Load the data\n",
    "data(mouse)\n",
    "head(mouse)\n",
    "\n",
    "\n",
    "# Extract gene expression data\n",
    "exprs <- mouse[1:25,c(\"M1\",\"M2\",\"M3\",\"NC1\",\"NC2\",\"NC3\")]\n",
    "rownames(exprs) <- mouse$ID[1:25]\n",
    "head(exprs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute clValid()\n",
    "\n",
    "We start by internal cluster validation which measures the connectivity, silhouette width and Dunn index. It’s possible to compute simultaneously these internal measures for multiple clustering algorithms in combination with a range of cluster numbers. The R code below can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clValid\n",
    "clmethods <- c(\"hierarchical\",\"kmeans\",\"pam\")\n",
    "intern <- clValid(exprs, nClust = 2:6,\n",
    "              clMethods = clmethods, validation = \"internal\")\n",
    "# Summary\n",
    "summary(intern)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that hierarchical clustering with two clusters performs the best in each case (i.e., for connectivity, Dunn and Silhouette measures).\n",
    "\n",
    "The plots of the connectivity, Dunn index, and silhouette width can be generated as follow:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(intern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Recall that the connectivity should be minimized, while both the Dunn index and the silhouette width should be maximized.\n",
    "\n",
    "Thus, it appears that hierarchical clustering outperforms the other clustering algorithms under each validation measure, for nearly every number of clusters evaluated.\n",
    "Regardless of the clustering algorithm, the optimal number of clusters seems to be two using the three measures.\n",
    "\n",
    " Stability measures can be computed as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability measures\n",
    "clmethods <- c(\"hierarchical\",\"kmeans\",\"pam\")\n",
    "stab <- clValid(exprs, nClust = 2:6, clMethods = clmethods,\n",
    "                validation = \"stability\")\n",
    "# Display only optimal Scores\n",
    "optimalScores(stab)\n",
    "\n",
    "#It’s also possible to display a complete summary:\n",
    "\n",
    "summary(stab)\n",
    "\n",
    "plot(stab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the APN and ADM measures, hierarchical clustering with two clusters again gives the best score. For the other measures, PAM with six clusters has the best score.\n",
    "\n",
    "For cluster biological validation read the documentation of clValid() (?clValid).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Summary\n",
    "\n",
    "We described how to validate clustering results using the silhouette method and the Dunn index. This task is facilitated using the combination of two R functions: eclust() and fviz_silhouette in the factoextra package We also demonstrated how to assess the agreement between a clustering result and an external reference.\n",
    "In the next chapters, we’ll show how to i) choose the appropriate clustering algorithm for your data; and ii) computing p-values for hierarchical clustering.\n",
    "\n",
    "\n",
    "## References\n",
    "*  Alboukadel Kassambara, 2017 \"Practical Guide to Cluster Analysis in R: Unsupervised Machine Learning\"\n",
    "\n",
    "* Brock, Guy, Vasyl Pihur, Susmita Datta, and Somnath Datta. 2008. “ClValid: An R Package for Cluster Validation.” Journal of Statistical Software 25 (4): 1–22. https://www.jstatsoft.org/v025/i04.\n",
    "\n",
    "* Charrad, Malika, Nadia Ghazzali, Véronique Boiteau, and Azam Niknafs. 2014. “NbClust: An R Package for Determining the Relevant Number of Clusters in a Data Set.” Journal of Statistical Software 61: 1–36. http://www.jstatsoft.org/v61/i06/paper.\n",
    "\n",
    "* Theodoridis, Sergios, and Konstantinos Koutroumbas. 2008. Pattern Recognition. 2nd ed. Academic Press.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
