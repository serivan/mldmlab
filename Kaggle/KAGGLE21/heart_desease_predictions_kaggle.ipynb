{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "How_to_submit_prediction_to_Kaggle_in_Python.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3bd4b4df-0cc0-40b1-b4da-c0b0dcb413f8",
        "_uuid": "26816927ad8b3a35dc55fafc33efd187f30cde39",
        "id": "j1-OG_fCcQyo",
        "tags": []
      },
      "source": [
        "####à#### # ##DDdddsdaòldsakòldaskòlHere you will learn to submit your model to a machine learning competition in Python.  It's fun, and it will give you a way to see your progress as your skills keep improving.*\n",
        "\n",
        "# Introduction\n",
        "Machine learning competitions are a great way to improve your skills and measure your progress as a data scientist. If you are using data from a competition on Kaggle, you can easily submit it from your notebook.  Here's how you do it.\n",
        "\n",
        "# Example\n",
        "We're doing very minimal data set up here so we can focus on how to submit modeling results to competitions. Other tutorials will teach you how build great models. So the model in this example will be fairly simple. We'll start with the code to read data, select predictors, and fit a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NgNXHNpcQyq",
        "outputId": "f626527e-63ff-4758-cb5d-3b13bf4345a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# if not 'sklearn' in sys.modules.keys():\n",
        "#    pip.main(['install', 'sklearn'])\n",
        "# if not 'kaggle' in sys.modules.keys():\n",
        "#    pip.main(['install', 'kaggle'])\n",
        "import random\n",
        "# import sys\n",
        "\n",
        "# import pip\n",
        "\n",
        "print(\"Random number with seed 2021\")\n",
        "# first call\n",
        "random.seed(2021)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random number with seed 2021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d0wiAEu_U4z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sP3hc6og6Q7n"
      },
      "source": [
        "#Insert here the description of your test in order to submit to Kaggle\n",
        "Description=\"First submission test \""
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUTtD0_d41Ph"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8524922d-be55-46fd-863f-004261fcfff0",
        "_uuid": "58cfd95aa5563209575b12977280983ffeea6492",
        "id": "79gMoBcZcQyy"
      },
      "source": [
        "import graphviz\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import (\n",
        "    GridSearchCV,\n",
        "    StratifiedKFold,\n",
        "    cross_val_score,\n",
        "    train_test_split,\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SPkn_9n41Pi"
      },
      "source": [
        "## Define Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXWtAa8x41Pi"
      },
      "source": [
        "DATA_PATH = \"https://raw.githubusercontent.com/serivan/mldmlab/master/Kaggle/KAGGLE21/\"\n",
        "OUTPUT_PATH = \"kaggle_submissions/\"\n",
        "RANDOM_STATE = 3993\n",
        "TRAIN_SIZE = 0.8"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INvHmTiN41Pi"
      },
      "source": [
        "## Define Utils"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhXvV6DlcQzA"
      },
      "source": [
        "Here we define a function for preperaing a submission file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPpDA5ElcQzB"
      },
      "source": [
        "def generateSubmission(myModel, submissionFile, description):\n",
        "    # Read the test data\n",
        "    data = pd.read_csv(DATA_PATH + \"test.csv\")\n",
        "\n",
        "    Id, X_test= data.Id,data.drop(columns=[\"Id\",\"target\"])\n",
        "    # Impute each test item, then predict\n",
        "    X_test_imp = imp.transform(X_test)\n",
        "\n",
        "    # Use the model to make predictions\n",
        "    y_test_pred = myModel.predict(X_test_imp)\n",
        "    # print(y_test_pred)\n",
        "\n",
        "    # submission file\n",
        "    my_submission = pd.DataFrame({\"Id\": Id, \"target\": y_test_pred})\n",
        "    # you could use any filename. We choose submission here\n",
        "    my_submission.to_csv(OUTPUT_PATH + submissionFile, index=False)\n",
        "\n",
        "    # Submit authomatically; kaggle API authentication needed\n",
        "    #!kaggle competitions submit -c heart-attack-analysis-and-prediction-21 -f {submissionFile} -m '{description}'"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAreCll741Pk"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8524922d-be55-46fd-863f-004261fcfff0",
        "_uuid": "58cfd95aa5563209575b12977280983ffeea6492",
        "id": "179PvzaY41Pk",
        "outputId": "97f877de-c32e-4590-a32d-138fb1a7437d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Read the data\n",
        "train = pd.read_csv(DATA_PATH + \"train.csv\")\n",
        "train.dtypes"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                       int64\n",
              "st_slope               float64\n",
              "age                    float64\n",
              "chest_pain_type        float64\n",
              "cholesterol            float64\n",
              "exercise_angina        float64\n",
              "fasting_blood_sugar    float64\n",
              "max_heart_rate         float64\n",
              "oldpeak                float64\n",
              "pulse                  float64\n",
              "resting_bp_s           float64\n",
              "resting_ecg            float64\n",
              "sex                    float64\n",
              "synt                   float64\n",
              "target                   int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlokqpsPcQy2",
        "outputId": "9ab5ea08-8d47-40df-815a-0ec639061fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>st_slope</th>\n",
              "      <th>age</th>\n",
              "      <th>chest_pain_type</th>\n",
              "      <th>cholesterol</th>\n",
              "      <th>exercise_angina</th>\n",
              "      <th>fasting_blood_sugar</th>\n",
              "      <th>max_heart_rate</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>pulse</th>\n",
              "      <th>resting_bp_s</th>\n",
              "      <th>resting_ecg</th>\n",
              "      <th>sex</th>\n",
              "      <th>synt</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>213.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>-0.051026</td>\n",
              "      <td>348.0</td>\n",
              "      <td>128.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.536459</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1001</td>\n",
              "      <td>2.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>1.626599</td>\n",
              "      <td>287.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.334897</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1002</td>\n",
              "      <td>2.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>183.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>0.968111</td>\n",
              "      <td>391.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.720858</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1003</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>179.0</td>\n",
              "      <td>0.021913</td>\n",
              "      <td>449.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.117560</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1004</td>\n",
              "      <td>2.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>-0.010970</td>\n",
              "      <td>258.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.790254</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Id  st_slope   age  chest_pain_type  ...  resting_ecg  sex      synt  target\n",
              "0  1000       1.0  62.0              2.0  ...          2.0  1.0  0.536459       0\n",
              "1  1001       2.0  72.0              3.0  ...          2.0  1.0  0.334897       0\n",
              "2  1002       2.0  49.0              3.0  ...          0.0  0.0  0.720858       1\n",
              "3  1003       1.0  35.0              2.0  ...          2.0  1.0  0.117560       0\n",
              "4  1004       2.0  51.0              4.0  ...          0.0  1.0  0.790254       1\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtiORdT141Pm"
      },
      "source": [
        "## Count missing values in columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBEC-jf_41Pm",
        "outputId": "c155a2b4-7408-4a55-b1a9-6be56a066a9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "number_of_missing_in_cols = train.shape[0] - train.count()\n",
        "number_of_missing_in_cols"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Id                       0\n",
              "st_slope                 1\n",
              "age                      4\n",
              "chest_pain_type          9\n",
              "cholesterol            202\n",
              "exercise_angina          3\n",
              "fasting_blood_sugar     11\n",
              "max_heart_rate           7\n",
              "oldpeak                  8\n",
              "pulse                    3\n",
              "resting_bp_s             7\n",
              "resting_ecg              3\n",
              "sex                      8\n",
              "synt                     0\n",
              "target                   0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_pwrwvZ41Pn"
      },
      "source": [
        "## Divide features by sematic type (binary, nominal, numerical)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUwIUeyv41Pn",
        "outputId": "fbfbc0ea-aaa1-4207-cea4-7cd1d034e791",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Count the number of unique values in dataframe\n",
        "cols_unique_vals_count = train.drop(columns=\"target\").nunique().sort_values()\n",
        "cols_unique_vals_count"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "exercise_angina           2\n",
              "fasting_blood_sugar       2\n",
              "sex                       2\n",
              "resting_ecg               3\n",
              "st_slope                  4\n",
              "chest_pain_type           4\n",
              "age                      49\n",
              "resting_bp_s             87\n",
              "max_heart_rate          121\n",
              "cholesterol             222\n",
              "pulse                   259\n",
              "oldpeak                 947\n",
              "Id                     1301\n",
              "synt                   1301\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-ehoc_241Pn",
        "outputId": "124e9dfb-4ec7-4f15-a773-4d406e91761c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# If the column has only 2 unique values it is a binary col\n",
        "BINARY_COLS = [col for col, val in cols_unique_vals_count.items() if val == 2]\n",
        "BINARY_COLS"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['exercise_angina', 'fasting_blood_sugar', 'sex']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxV7GdUF41Po",
        "outputId": "b2a2789f-5c37-49df-f62b-e8f961e2971f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Maximum number of unique values which represent a Nominal (categorical) feature\n",
        "NOMINAL_NUNIQUE_THRESHOLD = 10\n",
        "NOMINAL_COLS = [\n",
        "    col\n",
        "    for col, val in cols_unique_vals_count.items()\n",
        "    if val > 2 and val < NOMINAL_NUNIQUE_THRESHOLD\n",
        "]\n",
        "NOMINAL_COLS"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['resting_ecg', 'st_slope', 'chest_pain_type']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbtd2eAK41Po",
        "outputId": "6d836d95-91c4-415f-e8ff-2977a303d21c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Maximum number of unique values which represent a Nominal (categorical) feature\n",
        "NUMERICAL_COLS = [\n",
        "    col\n",
        "    for col, val in cols_unique_vals_count.items()\n",
        "    if val > NOMINAL_NUNIQUE_THRESHOLD\n",
        "]\n",
        "NUMERICAL_COLS"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['age',\n",
              " 'resting_bp_s',\n",
              " 'max_heart_rate',\n",
              " 'cholesterol',\n",
              " 'pulse',\n",
              " 'oldpeak',\n",
              " 'Id',\n",
              " 'synt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeR8YaEN41Pp",
        "outputId": "a8a6f55f-b2d2-4d4f-c6a6-cadc05d71dfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create the categorical columns list as binary + nominal\n",
        "CATEGORICAL_COLS = BINARY_COLS + NOMINAL_COLS\n",
        "CATEGORICAL_COLS"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['exercise_angina',\n",
              " 'fasting_blood_sugar',\n",
              " 'sex',\n",
              " 'resting_ecg',\n",
              " 'st_slope',\n",
              " 'chest_pain_type']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxX3XDIVcQy9"
      },
      "source": [
        "## Feature engineering phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY-ryLD15RKb"
      },
      "source": [
        "#Remove unuseful features\n",
        "train=train.drop(columns=\"Id\")"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbZ-89Mm41Pq"
      },
      "source": [
        "# Split dataset in data and target label\n",
        "X_train, y_train = train.drop(columns=\"target\"), train.target"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnkZf_fO41Pq"
      },
      "source": [
        "# Create categorical transformer (binary + nominal)\n",
        "# Replace imputer\n",
        "categorical_transformer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edZmLnYs41Pq"
      },
      "source": [
        "# Create numeric transformer\n",
        "# Impute missing values using the median value of each colum\n",
        "numeric_transformer = SimpleImputer(missing_values=np.nan, strategy=\"median\")"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GqvzLv641Pr"
      },
      "source": [
        "# Create the imputer of mixed type columns\n",
        "# ATTENTION: the preprocessor can be used in a Pipeline for simplicity\n",
        "imputer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"impute_numeric\", numeric_transformer, NUMERICAL_COLS),\n",
        "        (\"impute_categorical\", categorical_transformer, CATEGORICAL_COLS),\n",
        "    ],\n",
        "    verbose=True,\n",
        ")"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAGwdIWgcQy9",
        "outputId": "8418c05d-d481-4435-9eb1-629497598651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "source": [
        "# Fit the imputer\n",
        "imp = imputer.fit(X_train)\n",
        "# Impute our data\n",
        "X_train_imp = imp.transform(X_train)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mcolumn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mcolumn_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: 'Id' is not in list",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-158-2d6b7acc2df3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the imputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Impute our data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train_imp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;31m# we use fit_transform to make sure to set sparse_output_ (for which we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0;31m# need the transformed data) to have consistent output type in predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_transformers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_remainder\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mcols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mremaining_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_idx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 raise ValueError(\n\u001b[1;32m    467\u001b[0m                     \u001b[0;34m\"A given column is not a column of the dataframe\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 ) from e\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBgKH8fQ41Pu"
      },
      "source": [
        "# Column names after mixed imputation procedure\n",
        "COLUMN_NAMES = NUMERICAL_COLS + CATEGORICAL_COLS\n",
        "# Create a DataFrame from numpy array for visualization and simplicity\n",
        "X_train_imp = pd.DataFrame(data=X_train_imp, columns=COLUMN_NAMES)\n",
        "X_train_imp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xU8Pk2ScQzD"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edNIVyHAcQzE"
      },
      "source": [
        "### You can train directly on the training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaQjXaRMcQzE"
      },
      "source": [
        "my_model = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "my_model.fit(X_train_imp, y_train)\n",
        "my_model.score(X_train_imp, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ7a2eWW41Px"
      },
      "source": [
        "my_model.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL-VUzTFcQzH"
      },
      "source": [
        "# The snippet below will retrieve the feature importances from the model and make them into a DataFrame.\n",
        "feature_importances = pd.DataFrame(\n",
        "    data=my_model.feature_importances_,\n",
        "    index=X_train_imp.columns,\n",
        "    columns=[\"importance\"],\n",
        ").sort_values(\"importance\", ascending=False)\n",
        "feature_importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_flxO0mcQzJ"
      },
      "source": [
        "y_pred = my_model.predict(X_train_imp)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, y_pred))\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_train, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQWmFlljcQzL"
      },
      "source": [
        "# generate a submission file\n",
        "generateSubmission(my_model, \"first_decision_tree.csv\", Description)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuVsF4wacQzN"
      },
      "source": [
        "### You can consider different models splitting in training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXW11klacQzO"
      },
      "source": [
        "X_train_imp_2, X_test_imp_2, y_train_2, y_test_2 = train_test_split(\n",
        "    X_train_imp, y_train, train_size=TRAIN_SIZE, random_state=RANDOM_STATE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-e3K7nAPcQzQ"
      },
      "source": [
        "classification_tree = DecisionTreeClassifier(max_depth=3, random_state=RANDOM_STATE)\n",
        "classification_tree.fit(X_train_imp_2, y_train_2)\n",
        "classification_tree.score(X_train_imp_2, y_train_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eLhNrTgcQzT"
      },
      "source": [
        "classification_tree.score(X_test_imp_2, y_test_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzhSjoN0cQzV"
      },
      "source": [
        "# Visualize Decision Tree\n",
        "dot_data = export_graphviz(\n",
        "    decision_tree=classification_tree,\n",
        "    out_file=\"decision_tree_heart_desease.dot\",\n",
        "    feature_names=COLUMN_NAMES,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True,\n",
        ")\n",
        "\n",
        "# Read the dot graph and display image\n",
        "with open(\"decision_tree_heart_desease.dot\") as f:\n",
        "    dot_graph = f.read()\n",
        "\n",
        "# display(graphviz.Source(dot_graph))\n",
        "display(graphviz.Source(dot_graph))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vWqF8f5cQzX"
      },
      "source": [
        "# The snippet below will retrieve the feature importances from the model and make them into a DataFrame.\n",
        "feature_importances = pd.DataFrame(\n",
        "    classification_tree.feature_importances_,\n",
        "    index=COLUMN_NAMES,\n",
        "    columns=[\"importance\"],\n",
        ").sort_values(\"importance\", ascending=False)\n",
        "feature_importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81QSYVVccQza"
      },
      "source": [
        "y_pred_2 = classification_tree.predict(X_test_imp_2)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test_2, y_pred_2))\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test_2, y_pred_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wChhHNmfciQ6"
      },
      "source": [
        "classification_tree.fit(X_train_imp, y_train)\n",
        "classification_tree.score(X_train_imp, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkMPTgY4cQzc"
      },
      "source": [
        "# generate a submission file\n",
        "generateSubmission(\n",
        "    classification_tree,\n",
        "    \"second_decision_tree.csv\",\n",
        "    \"User defined decision tree evaluated with test set\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgGDn8RhcQze"
      },
      "source": [
        "### Or you can consider different models using cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnT5kDd1cQzg"
      },
      "source": [
        "dtc = DecisionTreeClassifier(max_depth=4, random_state=RANDOM_STATE)\n",
        "cv_scores = cross_val_score(\n",
        "    dtc, X_train_imp, y_train, cv=10, scoring=\"accuracy\", verbose=1\n",
        ")\n",
        "sns.distplot(cv_scores)\n",
        "plt.title(\"Average score: {}\".format(np.mean(cv_scores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkKGF9Socueq"
      },
      "source": [
        "dtc.fit(X_train_imp, y_train)\n",
        "dtc.score(X_train_imp, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omr6K-wLcQzj"
      },
      "source": [
        "# generate a submission file\n",
        "generateSubmission(\n",
        "    dtc,\n",
        "    \"crossval_decision_tree.csv\",\n",
        "    \"User defined decision tree evaluated with cross validation\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMIeoDe5cQzk"
      },
      "source": [
        "### Parameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C12hBTGcQzl"
      },
      "source": [
        "In every classification technique, there are some parameters that can be tuned to optimize the classification. Some parameters that can be tuned in the decision tree is max depth (the depth of the tree), max feature (the feature used to classify), criterion, and splitter.\n",
        "\n",
        "To search to tune parameter is to use Grid Search. Basically, it explores a range of parameters and finds the best combination of parameters. Then repeat the process several times until the best parameters are discovered. We will also use Stratified k-fold cross-validation that will prevent a certain class only split them to the same subset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAbnmTtocQzn"
      },
      "source": [
        "dtc = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "# Create the parameter grids\n",
        "parameter_grid = {\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "    \"splitter\": [\"best\", \"random\"],\n",
        "    \"max_depth\": [2, 3, 4, 5, 6, 7, 8],\n",
        "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
        "}\n",
        "\n",
        "# Create Stratified folds\n",
        "cross_validation = StratifiedKFold(n_splits=10)\n",
        "cross_validation.get_n_splits(X_train_imp, y_train)\n",
        "\n",
        "# Create the scoring dictionary\n",
        "SCORING = {\n",
        "    \"accuracy\": \"accuracy\",\n",
        "    \"balanced_accuracy\": \"balanced_accuracy\",\n",
        "    \"f1\": \"f1_macro\",\n",
        "}\n",
        "\n",
        "# Create and fit the GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dtc,\n",
        "    param_grid=parameter_grid,\n",
        "    cv=cross_validation,\n",
        "    verbose=1,\n",
        "    scoring=SCORING,\n",
        "    return_train_score=True,\n",
        "    refit=\"accuracy\",\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_imp, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3NhPvsCcQzo"
      },
      "source": [
        "# grid_search.cv_results_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmzMPoBDcQzq"
      },
      "source": [
        "print(\"Best score: {}\".format(grid_search.best_score_))\n",
        "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
        "\n",
        "best_dtc = grid_search.best_estimator_\n",
        "best_dtc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYHg4xQgcQzs"
      },
      "source": [
        "my_model = best_dtc\n",
        "my_model.fit(X_train_imp, y_train)\n",
        "my_model.score(X_train_imp, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YfuqF-ucQzu"
      },
      "source": [
        "y_pred = my_model.predict(X_train_imp)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_train, y_pred))\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_train, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ak6ohoZacQzw"
      },
      "source": [
        "# The snippet below will retrieve the feature importances from the model and make them into a DataFrame.\n",
        "feature_importances = pd.DataFrame(\n",
        "    my_model.feature_importances_, index=COLUMN_NAMES, columns=[\"importance\"]\n",
        ").sort_values(\"importance\", ascending=False)\n",
        "feature_importances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrK4RxylcQzy"
      },
      "source": [
        "### Submit the final model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8scvMiJicQzy"
      },
      "source": [
        "# generate a submission file\n",
        "generateSubmission(\n",
        "    my_model,\n",
        "    \"gridcv_decision_tree.csv\",\n",
        "    \"User defined decision tree evaluated with grid search\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "677b20af-bc56-4db1-aac3-1010389367ba",
        "_uuid": "c5294634157ca8c52d35c999ee949023b9544ee1",
        "id": "Vl2xROUscQz1"
      },
      "source": [
        "Step by step commands...\n",
        "\n",
        "In addition to your training data, there will be test data. This is frequently stored in a file with the title `test.csv`. This data won't include a column with your target (y), because that is what we'll have to predict and submit.  Here is sample code to do that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "a023518d-78e0-4e58-a243-6d1fe98992b9",
        "_uuid": "7bc843ef333847d305f576489d3e2aa3afc9384d",
        "id": "_TPmWWIUcQz1"
      },
      "source": [
        "# Read the test data\n",
        "X_test = pd.read_csv(DATA_PATH + \"test.csv\")\n",
        "\n",
        "# Impute each test item, then predict\n",
        "X_test_imp = imp.transform(X_test)\n",
        "\n",
        "# Use the model to make predictions\n",
        "y_test_pred = my_model.predict(X_test_imp)\n",
        "print(y_test_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "258e450a-d82c-4a7f-af22-cc695f74b4e3",
        "_uuid": "49e253dc359af831d8e248979b90c70dfcbef99a",
        "id": "EHZwo_p0cQz3"
      },
      "source": [
        "# Prepare Submission File\n",
        "We make submissions in CSV files.  Your submissions usually have two columns: an ID column and a prediction column.  The ID field comes from the test data (keeping whatever name the ID field had in that data, which for the housing data is the string 'Id'). The prediction column will use the name of the target field.\n",
        "\n",
        "We will create a DataFrame with this data, and then use the dataframe's `to_csv` method to write our submission file.  Explicitly include the argument `index=False` to prevent pandas from adding another column in our csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "092f5aed-0de4-4295-80ee-c193c8b84edd",
        "_uuid": "da5946c5fbc32fdb537788cd6ad5da1d6d698ae6",
        "id": "YWgl8n1IcQz3"
      },
      "source": [
        "# submission file\n",
        "my_submission = pd.DataFrame({\"test_index\": X_test.index, \"target\": y_test_pred})\n",
        "# you could use any filename. We choose submission here\n",
        "my_submission.to_csv(OUTPUT_PATH + \"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI90OYzMcQz5"
      },
      "source": [
        "# Make Submission\n",
        "Hit the blue **Publish** button at the top of your notebook screen.  It will take some time for your kernel to run.  When it has finished your navigation bar at the top of the screen will have a tab for **Output**.  This only shows up if you have written an output file (like we did in the **Prepare Submission File** step).  \n",
        "\n",
        "Otherwise, if you have a kaggle API token (https://www.kaggle.com/docs/api#getting-started-installation-&-authentication), you can use the following command (remove the '#'):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOiar0c4cQz5"
      },
      "source": [
        "# !kaggle competitions submit -c  heart-attack-analysis-and-prediction-21 -f kaggle_submissions/submission.csv -m \"Please describe the technique used\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ab2280a4-9c65-4810-8813-831f4dcd86e1",
        "_uuid": "97f80b7f76066fd78cdb5137757fb3c63abd0519",
        "id": "gFKeS8jBcQz7"
      },
      "source": [
        "\n",
        "# Last Steps \n",
        "Click on the Output button.  This will bring you to a screen with an option to **Submit to Competition**.  Hit that and you will see how your model performed.\n",
        "\n",
        "If you want to go back to improve your model, click the Edit button, which re-opens the kernel.  You'll need to re-run all the cells when you re-open the kernel.\n",
        "\n",
        "# Conclusion\n",
        "You've completed Level 1 of Machine Learning.  Congrats.  \n",
        "\n",
        "If you are ready to keep improving your model (and your skills), start level 2 of [Learn Machine Learning](https://www.kaggle.com/learn/machine-learning). \n",
        "\n",
        "Level 2 covers more powerful models, techniques to include non-numeric data, and more.  You can make more submissions to the competition and climb up the leaderboard as you go through the course.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtFv1-gV41P-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}